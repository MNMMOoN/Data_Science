# -*- coding: utf-8 -*-
"""tensorflow_image_helper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uL2mDIvcoPm8l3VV6-rs3uY-gBuAspUW
"""

# ========== Standard Library ==========
import os
import shutil
import requests
import datetime

# ========== Third-Party Libraries ==========
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics

# ========== TensorFlow ==========
import tensorflow as tf

class Load_Image_Data_with_URL:
    """
    A utility class to load image datasets from a ZIP file URL. It downloads, extracts,
    prepares training, validation, and test datasets using TensorFlow's image pipeline.

    Attributes:
        zipfile_url (str): URL to the zip file containing image data.
        dir_name (str): Name of the directory to extract files into (derived from zip file name).
        image_size (tuple): Size to resize images to, default is (224, 224).
        batch_size (int): Number of images per batch.
        validation_split (float): Proportion of training data to use for validation.
        rescale (bool): Whether to rescale image pixel values to [0, 1].
        verbose (bool): Whether to print status updates.
        train_dir (str or None): Path to the training data directory after extraction.
        test_dir (str or None): Path to the test data directory after extraction.
        train_data (tf.data.Dataset or None): The prepared training dataset.
        val_data (tf.data.Dataset or None): The prepared validation dataset.
        test_data (tf.data.Dataset or None): The prepared test dataset.
        class_names (list or None): List of class labels inferred from directory structure.
    """
    def __init__(self, zipfile_url, image_size=(224, 224), batch_size=32, validation_split=0.2, rescale=True, verbose=True):
        """
        Initializes the data loader with the given parameters.

        Args:
            zipfile_url (str): URL to the zip file containing image data.
            image_size (tuple): Target size for image resizing.
            batch_size (int): Number of images per batch.
            validation_split (float): Fraction of training data to use for validation.
            rescale (bool): Whether to rescale image pixel values to [0, 1].
            verbose (bool): If True, prints status messages.
        """
        self.zipfile_url = zipfile_url
        self.dir_name = zipfile_url.split('/')[-1].split('.')[0]
        self.image_size = image_size  # Default image size for resizing
        self.batch_size = batch_size  # Batch size for the dataset
        self.validation_split = validation_split  # Fraction for validation split
        self.rescale = rescale  # Boolean to control rescaling
        self.verbose = verbose  # Boolean to control printing of status messages

        self.train_dir = None
        self.test_dir = None
        self.train_data = None
        self.val_data = None
        self.test_data = None
        self.class_names = None

    def extract_data(self):
        """
        Downloads the zip file, extracts its contents, and loads the image datasets
        for training, validation, and testing using TensorFlow's image dataset utilities.

        Raises:
            RuntimeError: If directory creation or zip extraction fails.
            FileNotFoundError: If 'train' and 'test' subdirectories are not found.
        """

        # Download the zip file
        zipfile_name = self.zipfile_url.split('/')[-1]
        zipfile_name = zipfile_name + '.zip'
        if self.verbose:
            print(f"Downloading zip file from '{self.zipfile_url}'...")
        response = requests.get(self.zipfile_url)
        with open(zipfile_name, 'wb') as f:
            f.write(response.content)

        # Check if self.dir_name exists and is a file (not a directory)
        if os.path.exists(self.dir_name):
            if not os.path.isdir(self.dir_name):
                if self.verbose:
                    print(f"Removing existing file '{self.dir_name}' to create directory.")
                os.remove(self.dir_name)  # Remove file if it exists
            else:
                if self.verbose:
                    print(f"Directory '{self.dir_name}' already exists. Removing and recreating to avoid conflicts.")
                shutil.rmtree(self.dir_name)  # Remove existing directory and its contents

        # Create directory
        try:
            if self.verbose:
                print(f"Creating directory '{self.dir_name}'...")
            os.makedirs(self.dir_name, exist_ok=True)
        except Exception as e:
            raise RuntimeError(f"Failed to create directory '{self.dir_name}': {e}")

        # Extract the zip file silently
        try:
            if self.verbose:
                print(f"Extracting '{zipfile_name}' to '{self.dir_name}'...")
            os.system(f'unzip -q {zipfile_name} -d {self.dir_name}')
        except Exception as e:
            raise RuntimeError(f"Failed to extract zip file '{zipfile_name}' to '{self.dir_name}': {e}")

        # Remove the zip file after extraction
        try:
            if self.verbose:
                print(f"Removing zip file '{zipfile_name}'...")
            os.remove(zipfile_name)
        except Exception as e:
            if self.verbose:
                print(f"Warning: Could not remove zip file '{zipfile_name}': {e}")

        # Automatically locate train and test directories
        found_train, found_test = None, None
        for root, dirs, _ in os.walk(self.dir_name):
            for d in dirs:
                dir_lower = d.lower()
                if 'train' in dir_lower and not found_train:
                    found_train = os.path.join(root, d)
                elif 'test' in dir_lower and not found_test:
                    found_test = os.path.join(root, d)
            if found_train and found_test:
                break


        if not found_train or not found_test:
            raise FileNotFoundError(f"Could not find both 'train' and 'test' directories under extracted path '{self.dir_name}'.")

        self.train_dir = found_train
        self.test_dir = found_test

        if self.verbose:
            print()
            print(f"Found train directory at: {self.train_dir}")
            print(f"Found test directory at: {self.test_dir}")

        if self.verbose:
            print()
        # Load training data
        if self.verbose:
            print('Loading train data...')
        self.train_data = tf.keras.utils.image_dataset_from_directory(
            self.train_dir,
            validation_split=self.validation_split,
            subset="training",
            seed=42,
            image_size=self.image_size,
            batch_size=self.batch_size,
            shuffle=True,
            labels='inferred',
            label_mode='categorical'
        )

        if self.verbose:
            print()

        # Load validation data
        if self.verbose:
            print('Loading validation data...')
        self.val_data = tf.keras.utils.image_dataset_from_directory(
            self.train_dir,
            validation_split=self.validation_split,
            subset="validation",
            seed=42,
            image_size=self.image_size,
            batch_size=self.batch_size,
            shuffle=True,
            labels='inferred',
            label_mode='categorical'
        )

        if self.verbose:
            print()

        # Load test data
        if self.verbose:
            print('Loading test data...')
        self.test_data = tf.keras.utils.image_dataset_from_directory(
            self.test_dir,
            image_size=self.image_size,
            batch_size=self.batch_size,
            shuffle=False,
            labels='inferred',
            label_mode='categorical'
        )

        if self.verbose:
            print()

        self.class_names = self.train_data.class_names

        # Conditionally rescale images to [0, 1] if rescale=True
        if self.rescale:
            if self.verbose:
                print('Rescaling images to [0, 1]...')
            rescale_fn = lambda x, y: (x / 255.0, y)
            self.train_data = self.train_data.map(rescale_fn)
            self.val_data = self.val_data.map(rescale_fn)
            self.test_data = self.test_data.map(rescale_fn)

        if self.verbose:
            print('Data loading complete')

class ModelCallbacks:
    """
    A utility class for creating and managing common Keras training callbacks:
    - TensorBoard logging
    - Model checkpointing
    - Early stopping

    Attributes:
        tensorboard_callback (tf.keras.callbacks.TensorBoard): Callback for logging to TensorBoard.
        checkpoint_callback (tf.keras.callbacks.ModelCheckpoint): Callback for saving model checkpoints.
        early_stopping_callback (tf.keras.callbacks.EarlyStopping): Callback for early stopping during training.
    """

    def __init__(self, verbose=True):
        """
        Initialize the callback container.

        Args:
            verbose (bool): Whether to print status messages during setup.
        """
        self.tensorboard_callback = None
        self.checkpoint_callback = None
        self.early_stopping_callback = None
        self.verbose = verbose

    def tensorboard(self, dir_name, experiment_name):
        """
        Create and store a TensorBoard callback.

        Args:
            dir_name (str): Root directory where logs should be saved.
            experiment_name (str): Subfolder name to distinguish experiments.
        """
        log_dir = os.path.join(
            dir_name,
            experiment_name,
            datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        )
        self.tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

        if self.verbose:
            print(f"[TensorBoard] Log files will be saved to: {log_dir}")

    def checkpoint(self, dir_name, experiment_name, save_best_only=True, monitor='val_loss'):
        """
        Create and store a model checkpoint callback that saves only weights.

        Args:
            filepath (str): Filepath where the weights will be saved.
            save_best_only (bool): Whether to save only the best model (based on monitored metric).
            monitor (str): Metric to monitor for determining the best model.
        """
        # Creating a filepath
        filepath= os.path.join(
            dir_name,
            experiment_name,
            datetime.datetime.now().strftime("%Y%m%d-%H%M%S")+'.weights.h5'
        )

        # Make sure the directory exists
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=filepath,
            save_weights_only=True,  # <== Only weights, not full model
            save_best_only=save_best_only,
            monitor=monitor,
            verbose=1 if self.verbose else 0
        )
        if self.verbose:
            print(f"[Checkpoint] Saving only weights to: {filepath} (monitoring '{monitor}')")

    def early_stopping(self, patience=5, monitor='val_loss', restore_best_weights=True):
        """
        Create and store an early stopping callback.

        Args:
            patience (int): Number of epochs to wait for improvement before stopping.
            monitor (str): Metric to monitor for improvement.
            restore_best_weights (bool): Whether to restore model weights from the epoch with the best value of the monitored quantity.
        """
        self.early_stopping_callback = tf.keras.callbacks.EarlyStopping(
            patience=patience,
            monitor=monitor,
            restore_best_weights=restore_best_weights,
            verbose=1 if self.verbose else 0
        )
        if self.verbose:
            print(f"[EarlyStopping] Will stop if no improvement in '{monitor}' for {patience} epochs.")

    def get_callbacks(self):
        """
        Collect and return a list of all non-None callbacks.

        Returns:
            list: A list of keras.callbacks instances.
        """
        return [
            cb for cb in [
                self.tensorboard_callback,
                self.checkpoint_callback,
                self.early_stopping_callback
            ] if cb is not None
        ]

def RandomPlot(dataset, class_names, models=None, num_images=9):
    """
    Plots random images from a tf.data.Dataset with true labels and optional predictions
    from one or more models.

    Args:
        dataset (tf.data.Dataset): A batched image dataset.
        class_names (list): List of class names.
        models (list or tuple, optional): A list of Keras models for prediction. Can be None.
        num_images (int): Number of random images to display (should be a square like 9, 16).
    """
    # Get a batch of images and labels
    for images, labels in dataset.take(1):
        break

    total_images = images.shape[0]
    num_images = min(num_images, total_images)
    random_indices = np.random.choice(total_images, size=num_images, replace=False)

    # Get predictions from all models (if any)
    model_preds = []
    if models:
        for model in models:
            preds = model.predict(images, verbose=0)
            model_preds.append(preds)

    # Plotting
    images = tf.cast(images, tf.float32)
    if tf.reduce_max(images) > 1.0:
        images = images / 255.0

    grid_size = int(np.ceil(np.sqrt(num_images)))
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 4, grid_size * 4))
    axes = np.array(axes).reshape(grid_size, grid_size)

    for i, idx in enumerate(random_indices):
        row, col = divmod(i, grid_size)
        ax = axes[row, col]

        img = images[idx].numpy()
        true_label = class_names[np.argmax(labels[idx])]
        text_lines = [f"True: {true_label}"]

        for m_idx, preds in enumerate(model_preds):
            pred_label = class_names[np.argmax(preds[idx])]
            text_lines.append(f"Model {m_idx + 1}: {pred_label}")

        # Display image and title
        ax.imshow(img)
        ax.set_title("\n".join(text_lines), fontsize=10)
        ax.axis("off")

    # Hide unused subplots
    for j in range(num_images, grid_size * grid_size):
        row, col = divmod(j, grid_size)
        axes[row, col].axis("off")

    plt.suptitle("Random Images with Labels", fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

class Evaluation:
    """
    Evaluation class for TensorFlow models using test data.

    Attributes:
        model (tf.keras.Model): Trained model to be evaluated.
        test_data (tf.data.Dataset): Dataset for evaluation (batched).
        class_names (list): List of class names for label interpretation.
        confusion_matrix (ndarray): Confusion matrix (after evaluation).
        classification_report (str): Classification report (after evaluation).
        pred (ndarray): Predicted class indices.
        y_true_labels (ndarray): Ground-truth class indices.
    """

    def __init__(self, model, test_data, class_names, history= None, verbose=True):
        """
        Initialize the Evaluation class.

        Args:
            model (tf.keras.Model): Trained TensorFlow model to evaluate.
            test_data (tf.data.Dataset): Batched dataset containing test images and labels.
            class_names (list): List of class names corresponding to label indices.
            history (tf.keras.callbacks.History, optional): Training history object from model.fit().
                Used to plot training/validation loss and accuracy. Defaults to None.
            verbose (bool, optional): Whether to print progress messages. Defaults to True.
        """
        self.model = model
        self.history = history
        self.test_data = test_data
        self.class_names = class_names
        self.verbose = verbose

        self.confusion_matrix = None
        self.classification_report = None
        self.pred = None
        self.y_true_labels = None

    def _extract_true_labels(self):
        """Extracts true labels from the tf.data.Dataset object."""
        if self.verbose:
            print("[INFO] Extracting ground truth labels...")

        y_true = []
        for _, labels in self.test_data:
            y_true.extend(np.argmax(labels.numpy(), axis=1))  # one-hot to index
        self.y_true_labels = np.array(y_true)

    def _get_predictions(self):
        """Uses the model to predict on test data."""
        if self.verbose:
            print("[INFO] Making predictions...")

        y_pred = self.model.predict(self.test_data, verbose=0)
        self.pred = np.argmax(y_pred, axis=1)

    def plot_val_eval(self):
        if self.history is None:
            raise ValueError("Training history is not provided.")

        history_df = pd.DataFrame(self.history.history)

        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        if 'loss' in history_df.columns and 'val_loss' in history_df.columns:
            axes[0].plot(history_df['loss'], label='Training Loss')
            axes[0].plot(history_df['val_loss'], label='Validation Loss')
            axes[0].set_xlabel('Epochs')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
        else:
            axes[0].text(0.5, 0.5, "Loss data not available", ha='center')

        if 'accuracy' in history_df.columns and 'val_accuracy' in history_df.columns:
            axes[1].plot(history_df['accuracy'], label='Training Accuracy')
            axes[1].plot(history_df['val_accuracy'], label='Validation Accuracy')
            axes[1].set_xlabel('Epochs')
            axes[1].set_ylabel('Accuracy')
            axes[1].legend()
        else:
            axes[1].text(0.5, 0.5, "Accuracy data not available", ha='center')

        plt.tight_layout()
        plt.show()


    def plot_test_eval(self):
        """
        Evaluates the model on test data and visualizes performance:
        - Prints classification report
        - Displays confusion matrix with accuracy, precision, and recall
        """
        # Ensure predictions and labels are computed
        self._extract_true_labels()
        self._get_predictions()

        if self.verbose:
            print('\n====================')
            print('Model Evaluation')
            print('====================\n')

        # Classification report
        if self.verbose:
            print('====================')
            print('Classification Report')
            print('====================')
        self.classification_report = metrics.classification_report(
            self.y_true_labels, self.pred, target_names=self.class_names
        )
        print(self.classification_report)

        # Confusion Matrix
        if self.verbose:
            print('\n====================')
            print('Confusion Matrix')
            print('====================')
        self.confusion_matrix = metrics.confusion_matrix(self.y_true_labels, self.pred)

        accuracy = metrics.accuracy_score(self.y_true_labels, self.pred)
        precision = metrics.precision_score(self.y_true_labels, self.pred, average='weighted')
        recall = metrics.recall_score(self.y_true_labels, self.pred, average='weighted')

        disp = metrics.ConfusionMatrixDisplay(
            confusion_matrix=self.confusion_matrix,
            display_labels=self.class_names
        )

        fig, ax = plt.subplots(figsize=(8, 6))
        disp.plot(ax=ax, cmap='PuBu', values_format='d')
        ax.set_title(
            f'Confusion Matrix\nAccuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}'
        )
        ax.set_xticklabels(self.class_names, rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

class Model_with_BaseModel:
    """
    A class to build, compile, train, and evaluate a model based on a given base model (e.g., EfficientNetB0).

    Attributes:
        base_model (tf.keras.Model): A pre-trained base model without the top layer.
        input_shape (tuple): The shape of the input image, e.g., (224, 224, 3).
        output_shape (int): Number of output classes.
        model (tf.keras.Model): The final compiled model.
        history (History): Training history after model.fit().
        callbacks (ModelCallbacks): An instance of custom callbacks.
    """

    def __init__(self, base_model, input_shape, output_shape, verbose=True):
        """
        Initializes the model pipeline.

        Args:
            base_model (tf.keras.Model): A pre-trained model like EfficientNetB0 with include_top=False.
            input_shape (tuple): Input shape for the model.
            output_shape (int): Number of output classes.
            verbose (bool): Whether to print model summary and info.
        """
        self.base_model = base_model
        self.base_model.trainable = False  # Freeze base model initially
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.model = None
        self.history = None
        self.callbacks = None
        self.verbose = verbose

    def build_model(self):
        """
        Builds and compiles the full model using the base model.
        """

        augmentation = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=self.input_shape),
            tf.keras.layers.RandomFlip("horizontal"),
            tf.keras.layers.RandomFlip("vertical"),
            tf.keras.layers.RandomRotation(0.2),
            tf.keras.layers.RandomZoom(0.2),
            tf.keras.layers.RandomTranslation(0.2, 0.2)
        ], name="augmentation")

        inputs = tf.keras.layers.Input(shape=self.input_shape, name="input_layer")
        x = augmentation(inputs)
        x = self.base_model(inputs, training=False)
        x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)
        outputs = tf.keras.layers.Dense(self.output_shape, activation='softmax', name="output_layer")(x)

        self.model = tf.keras.Model(inputs, outputs, name="TransferLearningModel")
        self.model.compile(loss="categorical_crossentropy",
                           optimizer=tf.keras.optimizers.Adam(),
                           metrics=["accuracy"])

        if self.verbose:
            self.model.summary()
            print("Model compiled.\n\n")

    def setup_callbacks(self, dir_name="TensorBoard", experiment_name="Experiment"):
        """
        Initializes callbacks using ModelCallbacks class.

        Args:
            dir_name (str): Directory to store TensorBoard logs.
            experiment_name (str): Name of the experiment subdirectory.
        """
        self.callbacks = ModelCallbacks()
        self.callbacks.tensorboard(dir_name=dir_name, experiment_name=experiment_name)
        self.callbacks.early_stopping()
        self.callbacks.checkpoint(dir_name=dir_name, experiment_name=experiment_name)
        if self.verbose:
            print("Callbacks initialized.")

    def train(self, train_data, val_data, epochs=10):
        """
        Trains the model.

        Args:
            train_data (tf.data.Dataset): Batched training dataset.
            val_data (tf.data.Dataset): Batched validation dataset.
            epochs (int): Number of training epochs.
        """
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")

        if self.callbacks is None:
            self.setup_callbacks()

        self.history = self.model.fit(train_data,
                                      validation_data=val_data,
                                      epochs=epochs,
                                      steps_per_epoch=len(train_data),
                                      validation_steps=len(val_data),
                                      callbacks=self.callbacks.get_callbacks())
        if self.verbose:
            print("Training completed.")

    def evaluate(self, test_data, class_names, val_eval= False):
        """
        Evaluates the trained model using the Evaluation class.

        Args:
            test_data (tf.data.Dataset): Batched test dataset.
            class_names (list): List of class labels.
            val_eval (bool): Whether to evaluate on validation data. Default is False.
        """
        if self.model is None or self.history is None:
            raise ValueError("Model must be trained before evaluation.")

        eval = Evaluation(model=self.model,
                          test_data=test_data,
                          class_names=class_names,
                          history=self.history,
                          verbose=self.verbose)

        if val_eval:
            eval.plot_val_eval()

        eval.plot_test_eval()

    def plot_predictions(self, dataset, class_names, models = None, num_images=9):
        """
        Displays random image predictions.

        Args:
            dataset (tf.data.Dataset): Batched dataset to visualize predictions on.
            class_names (list): List of class labels.
            models (list): List of models to make predictions with. Default is an empty list.
            num_images (int): Number of images to display.
        """
        all_models = []  # Always include the base model

        if models:
            # Extract actual tf.keras.Model from wrapper classes if necessary
            extracted_models = []
            for m in models:
                if hasattr(m, 'model') and isinstance(m.model, tf.keras.Model):
                    extracted_models.append(m.model)
                elif isinstance(m, tf.keras.Model):
                    extracted_models.append(m)
                else:
                    raise TypeError(f"Unsupported model type: {type(m)}")
        all_models += extracted_models
        all_models.append(self.model)

        RandomPlot(dataset=dataset,
                  class_names=class_names,
                  models=all_models,
                  num_images=num_images)



