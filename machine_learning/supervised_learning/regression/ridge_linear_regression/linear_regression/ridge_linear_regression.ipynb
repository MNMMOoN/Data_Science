{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1>Regression Problems: Ridge linear regression</h1>\n",
        "    <h3>Mohammad Nourbakhsh Marvast</h3>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "FZXXqvhS8wJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O3PLfuwtRz4z"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd;\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ridge_reg:\n",
        "  def __init__(self, lr= 0.1, lambda_= 0.1,epochs= 10):\n",
        "    self.lr = lr\n",
        "    self.epochs = epochs\n",
        "    self.theta = None\n",
        "    self.lambda_ = lambda_\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
        "\n",
        "    n_samples, n_features = X_train.shape\n",
        "    self.theta = np.random.rand(n_features)\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      y_linear = np.dot(X_train, self.theta)\n",
        "\n",
        "      error = y_train - y_linear\n",
        "      #print(error)\n",
        "      gradient =  -(1/ n_samples) * np.dot(error, X_train) + self.lambda_ * self.theta\n",
        "      self.theta -= self.lr * gradient\n",
        "\n",
        "      # Calculate and print loss to monitor convergence\n",
        "      loss = self._compute_loss(X_train, y_train)\n",
        "      grad_norm = np.linalg.norm(gradient)\n",
        "      print(f\"Epoch {i+1}/{self.epochs}, Loss: {loss:.4f}, Gradient Norm: {grad_norm:.4f}\")\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    X_test = np.insert(X_test, 0, 1, axis=1)\n",
        "    return np.dot(X_test, self.theta)\n",
        "\n",
        "  def loss(self, y_pred, y_test):\n",
        "    n_samples = len(y_test)\n",
        "    regularization = 0.5 * self.lambda_ * np.dot(self.theta, self.theta)\n",
        "    return np.mean(0.5 * (y_test - y_pred) ** 2) + regularization\n",
        "\n",
        "  def _compute_loss(self, X, y):\n",
        "    # Internal loss for training data\n",
        "    y_pred = np.dot(X, self.theta)\n",
        "    n_samples = len(y)\n",
        "    regularization = 0.5 * self.lambda_ * np.dot(self.theta, self.theta)\n",
        "    return np.mean(0.5 * (y - y_pred) ** 2) + regularization"
      ],
      "metadata": {
        "id": "XX-VGzCXfWVT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## California Housing Dataset"
      ],
      "metadata": {
        "id": "pzUHOF9q1muq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing"
      ],
      "metadata": {
        "id": "xAX9wAqyhqSq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_frame = fetch_california_housing()\n",
        "data = data_frame.data\n",
        "target = data_frame.target"
      ],
      "metadata": {
        "id": "5WY4pkLshwuB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "yPTQW2prh5hF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = ridge_reg(lr= 0.5, lambda_= 0.1,epochs= 10)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WizLUZ9Nh8qu",
        "outputId": "0d64bb0c-235b-4465-e69a-58cf7e53957a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.9271, Gradient Norm: 2.1310\n",
            "Epoch 2/10, Loss: 0.7052, Gradient Norm: 0.7753\n",
            "Epoch 3/10, Loss: 0.6453, Gradient Norm: 0.3882\n",
            "Epoch 4/10, Loss: 0.6193, Gradient Norm: 0.2438\n",
            "Epoch 5/10, Loss: 0.6025, Gradient Norm: 0.1902\n",
            "Epoch 6/10, Loss: 0.5896, Gradient Norm: 0.1653\n",
            "Epoch 7/10, Loss: 0.5790, Gradient Norm: 0.1490\n",
            "Epoch 8/10, Loss: 0.5702, Gradient Norm: 0.1357\n",
            "Epoch 9/10, Loss: 0.5628, Gradient Norm: 0.1240\n",
            "Epoch 10/10, Loss: 0.5567, Gradient Norm: 0.1134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-GTBusRoA8i",
        "outputId": "6c9cd596-9ff8-4529-e6a6-0397fb5a5ff2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.97200164 1.37759337 2.08503129 ... 3.88484033 1.56826059 1.60004223]\n",
            "[0.477   0.458   5.00001 ... 5.00001 0.723   1.515  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "print(f\"\\nTest Loss: {clf.loss(y_pred, y_test):.4f}\")\n",
        "print(f\"Test MSE: {np.mean((y_test - y_pred) ** 2):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_dLaTvkiEX5",
        "outputId": "2c811313-b6c6-49e6-ef38-eae6258e59a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.5610\n",
            "Test MSE: 0.7027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boston Housing Dataset"
      ],
      "metadata": {
        "id": "2AhcB9K812Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boston_url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
        "boston_df = pd.read_csv(boston_url)\n",
        "X_bos = boston_df.drop(\"medv\", axis=1).values\n",
        "y_bos = boston_df[\"medv\"].values"
      ],
      "metadata": {
        "id": "TyVGCdYpnoGk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test =  train_test_split(X_bos, y_bos, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NXHSX-cN18wZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = ridge_reg(lr= 0.1, lambda_= 0.001,epochs=100)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3VRdhX42yrv",
        "outputId": "dd8c7fd6-7017-4e73-9e58-42d6aa962b4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 237.2047, Gradient Norm: 30.5031\n",
            "Epoch 2/100, Loss: 190.7284, Gradient Norm: 22.4824\n",
            "Epoch 3/100, Loss: 155.9440, Gradient Norm: 19.2091\n",
            "Epoch 4/100, Loss: 128.3045, Gradient Norm: 17.0759\n",
            "Epoch 5/100, Loss: 106.0606, Gradient Norm: 15.3097\n",
            "Epoch 6/100, Loss: 88.1072, Gradient Norm: 13.7519\n",
            "Epoch 7/100, Loss: 73.6034, Gradient Norm: 12.3595\n",
            "Epoch 8/100, Loss: 61.8800, Gradient Norm: 11.1112\n",
            "Epoch 9/100, Loss: 52.3998, Gradient Norm: 9.9913\n",
            "Epoch 10/100, Loss: 44.7301, Gradient Norm: 8.9862\n",
            "Epoch 11/100, Loss: 38.5224, Gradient Norm: 8.0841\n",
            "Epoch 12/100, Loss: 33.4956, Gradient Norm: 7.2742\n",
            "Epoch 13/100, Loss: 29.4231, Gradient Norm: 6.5470\n",
            "Epoch 14/100, Loss: 26.1220, Gradient Norm: 5.8941\n",
            "Epoch 15/100, Loss: 23.4447, Gradient Norm: 5.3077\n",
            "Epoch 16/100, Loss: 21.2719, Gradient Norm: 4.7811\n",
            "Epoch 17/100, Loss: 19.5073, Gradient Norm: 4.3082\n",
            "Epoch 18/100, Loss: 18.0732, Gradient Norm: 3.8836\n",
            "Epoch 19/100, Loss: 16.9067, Gradient Norm: 3.5023\n",
            "Epoch 20/100, Loss: 15.9568, Gradient Norm: 3.1599\n",
            "Epoch 21/100, Loss: 15.1825, Gradient Norm: 2.8525\n",
            "Epoch 22/100, Loss: 14.5505, Gradient Norm: 2.5767\n",
            "Epoch 23/100, Loss: 14.0339, Gradient Norm: 2.3292\n",
            "Epoch 24/100, Loss: 13.6110, Gradient Norm: 2.1072\n",
            "Epoch 25/100, Loss: 13.2639, Gradient Norm: 1.9081\n",
            "Epoch 26/100, Loss: 12.9786, Gradient Norm: 1.7297\n",
            "Epoch 27/100, Loss: 12.7434, Gradient Norm: 1.5699\n",
            "Epoch 28/100, Loss: 12.5490, Gradient Norm: 1.4268\n",
            "Epoch 29/100, Loss: 12.3878, Gradient Norm: 1.2989\n",
            "Epoch 30/100, Loss: 12.2535, Gradient Norm: 1.1845\n",
            "Epoch 31/100, Loss: 12.1413, Gradient Norm: 1.0824\n",
            "Epoch 32/100, Loss: 12.0471, Gradient Norm: 0.9914\n",
            "Epoch 33/100, Loss: 11.9676, Gradient Norm: 0.9102\n",
            "Epoch 34/100, Loss: 11.9001, Gradient Norm: 0.8380\n",
            "Epoch 35/100, Loss: 11.8424, Gradient Norm: 0.7738\n",
            "Epoch 36/100, Loss: 11.7929, Gradient Norm: 0.7167\n",
            "Epoch 37/100, Loss: 11.7500, Gradient Norm: 0.6662\n",
            "Epoch 38/100, Loss: 11.7127, Gradient Norm: 0.6213\n",
            "Epoch 39/100, Loss: 11.6799, Gradient Norm: 0.5816\n",
            "Epoch 40/100, Loss: 11.6509, Gradient Norm: 0.5465\n",
            "Epoch 41/100, Loss: 11.6250, Gradient Norm: 0.5154\n",
            "Epoch 42/100, Loss: 11.6018, Gradient Norm: 0.4879\n",
            "Epoch 43/100, Loss: 11.5809, Gradient Norm: 0.4635\n",
            "Epoch 44/100, Loss: 11.5618, Gradient Norm: 0.4418\n",
            "Epoch 45/100, Loss: 11.5443, Gradient Norm: 0.4226\n",
            "Epoch 46/100, Loss: 11.5282, Gradient Norm: 0.4054\n",
            "Epoch 47/100, Loss: 11.5132, Gradient Norm: 0.3901\n",
            "Epoch 48/100, Loss: 11.4993, Gradient Norm: 0.3764\n",
            "Epoch 49/100, Loss: 11.4862, Gradient Norm: 0.3640\n",
            "Epoch 50/100, Loss: 11.4740, Gradient Norm: 0.3527\n",
            "Epoch 51/100, Loss: 11.4624, Gradient Norm: 0.3425\n",
            "Epoch 52/100, Loss: 11.4515, Gradient Norm: 0.3332\n",
            "Epoch 53/100, Loss: 11.4410, Gradient Norm: 0.3246\n",
            "Epoch 54/100, Loss: 11.4311, Gradient Norm: 0.3167\n",
            "Epoch 55/100, Loss: 11.4217, Gradient Norm: 0.3093\n",
            "Epoch 56/100, Loss: 11.4126, Gradient Norm: 0.3024\n",
            "Epoch 57/100, Loss: 11.4040, Gradient Norm: 0.2959\n",
            "Epoch 58/100, Loss: 11.3957, Gradient Norm: 0.2898\n",
            "Epoch 59/100, Loss: 11.3877, Gradient Norm: 0.2840\n",
            "Epoch 60/100, Loss: 11.3800, Gradient Norm: 0.2785\n",
            "Epoch 61/100, Loss: 11.3726, Gradient Norm: 0.2733\n",
            "Epoch 62/100, Loss: 11.3655, Gradient Norm: 0.2683\n",
            "Epoch 63/100, Loss: 11.3586, Gradient Norm: 0.2634\n",
            "Epoch 64/100, Loss: 11.3519, Gradient Norm: 0.2588\n",
            "Epoch 65/100, Loss: 11.3455, Gradient Norm: 0.2543\n",
            "Epoch 66/100, Loss: 11.3393, Gradient Norm: 0.2500\n",
            "Epoch 67/100, Loss: 11.3333, Gradient Norm: 0.2458\n",
            "Epoch 68/100, Loss: 11.3275, Gradient Norm: 0.2417\n",
            "Epoch 69/100, Loss: 11.3219, Gradient Norm: 0.2377\n",
            "Epoch 70/100, Loss: 11.3165, Gradient Norm: 0.2339\n",
            "Epoch 71/100, Loss: 11.3113, Gradient Norm: 0.2301\n",
            "Epoch 72/100, Loss: 11.3062, Gradient Norm: 0.2265\n",
            "Epoch 73/100, Loss: 11.3012, Gradient Norm: 0.2229\n",
            "Epoch 74/100, Loss: 11.2965, Gradient Norm: 0.2195\n",
            "Epoch 75/100, Loss: 11.2918, Gradient Norm: 0.2161\n",
            "Epoch 76/100, Loss: 11.2873, Gradient Norm: 0.2127\n",
            "Epoch 77/100, Loss: 11.2830, Gradient Norm: 0.2095\n",
            "Epoch 78/100, Loss: 11.2788, Gradient Norm: 0.2063\n",
            "Epoch 79/100, Loss: 11.2747, Gradient Norm: 0.2032\n",
            "Epoch 80/100, Loss: 11.2707, Gradient Norm: 0.2002\n",
            "Epoch 81/100, Loss: 11.2668, Gradient Norm: 0.1972\n",
            "Epoch 82/100, Loss: 11.2631, Gradient Norm: 0.1943\n",
            "Epoch 83/100, Loss: 11.2594, Gradient Norm: 0.1914\n",
            "Epoch 84/100, Loss: 11.2559, Gradient Norm: 0.1886\n",
            "Epoch 85/100, Loss: 11.2525, Gradient Norm: 0.1859\n",
            "Epoch 86/100, Loss: 11.2491, Gradient Norm: 0.1832\n",
            "Epoch 87/100, Loss: 11.2459, Gradient Norm: 0.1806\n",
            "Epoch 88/100, Loss: 11.2428, Gradient Norm: 0.1780\n",
            "Epoch 89/100, Loss: 11.2397, Gradient Norm: 0.1755\n",
            "Epoch 90/100, Loss: 11.2367, Gradient Norm: 0.1730\n",
            "Epoch 91/100, Loss: 11.2338, Gradient Norm: 0.1706\n",
            "Epoch 92/100, Loss: 11.2310, Gradient Norm: 0.1682\n",
            "Epoch 93/100, Loss: 11.2283, Gradient Norm: 0.1658\n",
            "Epoch 94/100, Loss: 11.2256, Gradient Norm: 0.1636\n",
            "Epoch 95/100, Loss: 11.2231, Gradient Norm: 0.1613\n",
            "Epoch 96/100, Loss: 11.2205, Gradient Norm: 0.1591\n",
            "Epoch 97/100, Loss: 11.2181, Gradient Norm: 0.1569\n",
            "Epoch 98/100, Loss: 11.2157, Gradient Norm: 0.1548\n",
            "Epoch 99/100, Loss: 11.2134, Gradient Norm: 0.1527\n",
            "Epoch 100/100, Loss: 11.2111, Gradient Norm: 0.1507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "print(f\"\\nTest Loss: {clf.loss(y_pred, y_test):.4f}\")\n",
        "print(f\"Test MSE: {np.mean((y_test - y_pred) ** 2):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT3XLEOq21cw",
        "outputId": "8f3ab989-f7d2-4950-a56d-532d14fc2700"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 12.9538\n",
            "Test MSE: 25.3476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "giJ5vlSp3LUS"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}
